# AIProxy Development Guide

This document provides guidelines and instructions for agents working on this codebase.

## Project Overview

AIProxy is a two-tier system that forwards OpenAI-compatible API requests to browser-based clients:
- **Backend** (Python/FastAPI): HTTP server accepting OpenAI API requests, forwarding to WebSocket clients
- **Frontend** (Tampermonkey userscript): Browser clients that inject into AI chat platforms (ChatGPT, Claude, Yuanbao, Arena, etc.)

### System Architecture

```
┌─────────────────┐         ┌──────────────────┐         ┌─────────────────┐
│  Client App     │ HTTP    │  Backend Server  │ WebSocket│  Browser Client │
│  (OpenAI API)   │───────▶ │  (FastAPI)       │─────────▶│  (Userscript)   │
│                 │         │                  │         │                 │
│  - OpenAI SDK   │         │  - /v1/chat/     │         │  - Injects into │
│  - curl/requests│         │    completions   │         │    ChatGPT      │
│  - Any HTTP     │         │  - WebSocket     │         │  - Claude.ai    │
│                 │         │    /ws           │         │  - Yuanbao      │
└─────────────────┘         │  - Connection    │         │  - Arena.ai     │
                            │    Manager       │         │                 │
                            └──────────────────┘         └─────────────────┘
```

## Project Structure

```
AIProxy/
├── backend/                    # Python FastAPI backend
│   ├── main.py               # FastAPI application, HTTP endpoints
│   ├── websocket_manager.py   # WebSocket connection management
│   └── requirements.txt       # Python dependencies
├── debug_logs/               # Debug log files (auto-created at project root)
├── js/                        # JavaScript frontend
│   ├── main.js               # Production userscript (auto-generated)
│   ├── package.json          # npm dependencies and scripts
│   ├── src/                  # Source code (edit here, not main.js)
│   │   ├── modules/          # Modular source files
│   │   │   ├── config.js    # Website selectors and configuration
│   │   │   ├── utils.js     # Utility functions
│   │   │   ├── websocketManager.js  # WebSocket client
│   │   │   ├── domManager.js      # DOM manipulation
│   │   │   └── aiChatForwarder.js  # Main orchestration logic
│   │   ├── tests/            # Jest unit tests
│   │   │   ├── config.test.js
│   │   │   ├── utils.test.js
│   │   │   ├── websocketManager.test.js
│   │   │   ├── domManager.test.js
│   │   │   └── aiChatForwarder.test.js
│   │   ├── html/             # Test HTML templates
│   │   │   └── test.html
│   │   └── build.js          # Build script (merges modules)
│   └── TESTING.md            # Frontend testing guide
├── tasks/                     # Documentation and design docs
│   ├── refactor-design.md
│   ├── refactor-message-container-selector.md
│   └── js_system_role_support.md
├── build-main.py             # Main build script (from project root)
├── test_backend.py           # Backend integration tests
├── test_avante_stream.py     # Stream mode test script (simulates avante.nvim requests)
├── test-js.py                # Frontend test runner
├── AGENTS.md                 # This file - development guide
└── .github/
    └── copilot-instructions.md
```

## Environment Setup

### Prerequisites

- **Python 3.10+**: For the backend server
- **Node.js 16+**: For building and testing the frontend
- **Tampermonkey**: Browser extension for running the userscript

### Backend Setup

```bash
# Create virtual environment (recommended)
cd backend
python -m venv venv

# Activate virtual environment
# Windows:
venv\Scripts\activate
# macOS/Linux:
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Run the backend server
python main.py
# Server will start on http://localhost:8000
```

### Frontend Setup

```bash
# Navigate to js directory
cd js

# Install npm dependencies
npm install

# Build the production userscript
npm run build
# Or from project root: python build-main.py

# Run tests
npm test
```

### Browser Setup

1. Install Tampermonkey extension in your browser
2. Open `js/main.js` (generated by build process)
3. Copy the entire content
4. In Tampermonkey, create a new script and paste the content
5. Save the script
6. Navigate to a supported AI platform (ChatGPT, Claude, etc.)
7. The userscript will automatically connect to the backend server

## Build/Lint/Test Commands

### Backend (Python)

```bash
# Install dependencies
pip install -r backend/requirements.txt

# Run backend server
python backend/main.py
# Or: uvicorn main:app --reload (from backend/ directory)

# Run backend tests (when available)
python -m pytest backend/ -v
python -m pytest backend/test_specific.py::TestClass::test_method -v
```

### Frontend (JavaScript)

```bash
# Install npm dependencies
cd js && npm install

# Run tests
npm test                              # Run all tests
npm run test -- --testNamePattern="test name"  # Run specific test
npm run test -- file.test.js          # Run specific test file
npm run test:watch                    # Watch mode
npm run test:coverage                 # Coverage report

# Build production userscript
npm run build
# Or: python build-main.py (from root)
# Or: node js/src/build.js
```

### All-in-One

```bash
# Build frontend and run tests
python build-main.py && python test-js.py
```

### Test Scripts

The project includes several test scripts for different testing scenarios:

#### test_backend.py
Backend integration tests that verify end-to-end functionality with a connected browser client.

```bash
# Run backend integration tests
python test_backend.py
```

#### test_avante_stream.py
Stream mode test script that simulates avante.nvim requests to the backend. Supports both streaming and non-streaming modes.

```bash
# Run basic non-streaming test
python test_avante_stream.py

# Run streaming test
python test_avante_stream.py --stream

# Run with custom task
python test_avante_stream.py --task "Write hello world"

# Run with custom model
python test_avante_stream.py --model gpt-4

# Run health check only
python test_avante_stream.py --health

# Run all tests
python test_avante_stream.py --all
```

**Features:**
- Simulates avante.nvim-style requests with proper message format
- Supports streaming responses (SSE) and non-streaming responses
- Tests tool calls handling
- Includes health check endpoint testing
- Provides detailed response formatting and error reporting
- Validates OpenAI API compatibility

**Use Cases:**
- Test streaming functionality without needing a full client
- Verify tool calls are properly handled
- Debug streaming response parsing issues
- Test with custom prompts and models
- Run comprehensive test suite for backend validation

## Code Style Guidelines

### Python (Backend)

**Imports:**
- Standard library imports first, then third-party, then local
- Use absolute imports from package root
- Group imports by type with blank lines between groups

```python
# Correct order
import logging
import uuid
import json
from datetime import datetime
from typing import Dict, List, Optional

from fastapi import FastAPI, WebSocket
from pydantic import BaseModel

from websocket_manager import connection_manager
```

**Formatting:**
- Line length: 100 characters (soft limit)
- Use 4 spaces for indentation
- Use trailing commas in multi-line structures
- Use blank lines to separate logical sections within functions
- Use Unix-style line endings (`\n`) instead of Windows-style (`\r\n`)

**Types:**
- Use type hints for function parameters and return values
- Use `Optional[T]` instead of `Union[T, None]`
- Use `List[T]`, `Dict[K, V]` from typing module
- Define Pydantic models for all API request/response schemas

**Naming Conventions:**
- `snake_case` for functions, variables, and module names
- `PascalCase` for classes and Pydantic models
- `UPPER_SNAKE_CASE` for constants
- Prefix private methods with underscore: `_private_method()`
- Descriptive names: `get_available_client()` not `get_client()`

**Error Handling:**
- Use try/except with specific exception types
- Log errors with `logger.error()` before re-raising
- Return meaningful error messages in API responses
- Use HTTPException for HTTP errors, custom exceptions for domain errors
- Always clean up resources in finally blocks

```python
try:
    result = await process_request(request)
    return result
except ValueError as e:
    logger.error(f"Invalid request: {e}")
    raise HTTPException(status_code=400, detail=str(e))
except Exception as e:
    logger.error(f"Unexpected error: {e}")
    raise HTTPException(status_code=500, detail="Internal server error")
```

**Async/Await:**
- Use `async def` for all FastAPI endpoint handlers
- Use `asyncio.Lock()` for thread-safe operations on shared state
- Never block in async functions (use async libraries or `asyncio.to_thread()`)
- Always set timeouts on async operations

**Logging:**
- Use module-level logger: `logger = logging.getLogger(__name__)`
- Log at appropriate levels: `debug`, `info`, `warning`, `error`
- Include request IDs and client IDs in log messages

### JavaScript (Frontend)

**Formatting:**
- Use ESLint/Prettier conventions (2 spaces, semicolons, double quotes)
- Use `const` by default, `let` when reassignment is needed
- Use template literals for string interpolation

**Types (JSDoc):**
- Document functions with JSDoc comments
- Include @param and @return types
- Describe behavior, not just types

**Naming Conventions:**
- `camelCase` for variables and functions
- `PascalCase` for classes
- `SCREAMING_SNAKE_CASE` for constants
- Descriptive names: `handleCompletionRequest()` not `handleReq()`

**Error Handling:**
- Always handle promise rejections
- Use try/catch in async functions
- Log errors with `console.error()` or `console.warn()`

**Module Structure:**
- Each module in `js/src/modules/` should have a single responsibility
- Use ES6 classes for stateful components
- Use pure functions where possible

## Architecture Patterns

### Backend (main.py)

- **Pydantic Models**: Define request/response schemas matching OpenAI API format
- **ConnectionManager**: Central hub for WebSocket client state and routing
- **Request/Response Matching**: Uses `request_id` to correlate HTTP requests with WebSocket responses
- **Heartbeat System**: 25s interval, 30s timeout for client health checks

### Frontend (main.js)

- **Source Modules**: Edit in `js/src/modules/`, never directly in `js/main.js`
- **Build Process**: `build-main.py` merges modules, removes imports/exports, wraps in IIFE
- **Platform Adapters**: Site-specific selectors in `config.js` for each AI platform

## Message Contracts

### HTTP → WebSocket (server → client)
```json
{"type": "completion_request", "request_id": "req_...", "model": "...", "messages": [...], "temperature": 0.7}
```

### WebSocket → HTTP (client → server)
```json
{"type": "completion_response", "request_id": "req_...", "content": "...", "finish_reason": "stop"}
```

### Heartbeat
```json
{"type": "heartbeat"}           // server → client
{"type": "heartbeat_response"}  // client → server
```

## Key Files

| Path | Purpose |
|------|---------|
| `backend/main.py` | FastAPI app, HTTP endpoints, WebSocket handler |
| `backend/websocket_manager.py` | Client connection state, request routing |
| `js/src/modules/config.js` | Site-specific CSS selectors |
| `js/src/modules/websocketManager.js` | WebSocket client connection handling |
| `js/src/modules/domManager.js` | DOM manipulation for AI chat UIs |
| `js/src/modules/aiChatForwarder.js` | Main orchestration logic |
| `js/main.js` | Production userscript (auto-generated) |
| `build-main.py` | Builds js/main.js from source modules |

## Development Workflow

### Backend Changes
1. Edit files in `backend/`
2. Server auto-reloads with `python backend/main.py`
3. Wait for user to deploy frontend changes to browser before running tests
4. Run `python test_backend.py` only after confirming browser has latest userscript

### Frontend Changes
1. Edit source modules in `js/src/modules/` (NOT main.js)
2. Run `python build-main.py` to regenerate `js/main.js`
3. Verify build output (check line count, critical methods)
4. Deploy: Copy `js/main.js` content to Tampermonkey
5. Refresh browser page to load new userscript
6. Test with `python test_backend.py` to verify end-to-end functionality

## Testing Guidelines

### Writing Backend Tests
- Use `pytest` framework
- Place tests in `backend/tests/` directory
- Mock WebSocket connections for isolated testing
- Test error handling paths thoroughly

### Writing Frontend Tests
- Use Jest framework with jsdom environment
- Place tests in `js/src/tests/` alongside modules
- Test individual module functions in isolation
- Mock DOM elements for DOM-related tests

## API Endpoints

### Backend HTTP Endpoints

#### OpenAI-Compatible Endpoints

| Method | Endpoint | Description |
|--------|-----------|-------------|
| POST | `/v1/chat/completions` | OpenAI chat completion API - forwards requests to WebSocket clients |
| GET | `/v1/models` | Returns supported models list (OpenAI-compatible) |

#### Management Endpoints

| Method | Endpoint | Description |
|--------|-----------|-------------|
| GET | `/` | Root endpoint - returns service status and connection stats |
| GET | `/health` | Health check - returns service health status |
| GET | `/stats` | Get connection statistics |
| GET | `/logs` | List all debug log files |
| GET | `/logs/{filename}` | Get content of a specific log file |
| DELETE | `/logs` | Clear all debug log files |

#### WebSocket Endpoint

| Method | Endpoint | Description |
|--------|-----------|-------------|
| WS | `/ws` | WebSocket endpoint for browser client connections |

### Debugging Features

The backend includes comprehensive debugging capabilities:

#### Debug Logs
- All requests and responses are saved to `debug_logs/` directory (at project root)
- Each request generates multiple files:
  - `{request_id}_request.json` - Original OpenAI request
  - `{request_id}_forward.json` - Request forwarded to client
  - `{request_id}_response.json` - Client's raw response
  - `{request_id}_openai_response.json` - Formatted OpenAI response
  - `{request_id}_tool_calls.json` - Tool calls (if present)
  - `{client_id}_*.log` - Client-side logs forwarded to server

#### Viewing Logs
```bash
# List all log files
curl http://localhost:8000/logs

# Get specific log file content
curl http://localhost:8000/logs/req_abc123_request.json

# Clear all logs
curl -X DELETE http://localhost:8000/logs
```

#### Client Logging
The frontend can send logs to the backend via WebSocket:
```json
{
  "type": "client_log",
  "level": "info|warning|error",
  "category": "websocket|dom|forwarder",
  "message": "Log message",
  "data": { ... }
}
```

## Supported Platforms

The userscript supports the following AI platforms:

| Platform | Domain | Status |
|----------|--------|--------|
| ChatGPT | `chat.openai.com` | ✅ Supported |
| Claude | `claude.ai` | ✅ Supported |
| Arena.ai | `arena.ai` | ✅ Supported |
| Yuanbao (腾讯元宝) | `yuanbao.tencent.com` | ✅ Supported |
| Other | (fallback) | ⚠️ Basic support |

### Platform Configuration

Platform-specific CSS selectors are defined in `js/src/modules/config.js`. To add support for a new platform:

1. Add a new entry to `WEBSITE_SELECTORS` object:
```javascript
'newplatform.com': {
    inputBox: ['selector-for-input'],
    sendButton: ['selector-for-send-button'],
    pageReadyIndicator: ['selector-indicating-page-ready'],
    messageListContainer: ['selector-for-message-container'],
    latestMessage: ['selector-for-latest-ai-message']
}
```

2. The `getCurrentSiteConfig()` function automatically matches by hostname
3. Test the selectors in the browser's DevTools before committing

### Configuration Options

Frontend configuration in `js/src/modules/config.js`:

```javascript
export const CONFIG = {
    // WebSocket server address
    wsServer: 'ws://localhost:8000/ws',

    // Timeout settings (milliseconds)
    timeouts: {
        elementWait: 10000,    // Wait for element to appear
        messageSend: 30000,     // Message send timeout
        responseWait: 120000,   // Response wait timeout
        reconnect: 5000         // Reconnect interval
    },

    // Retry configuration
    retry: {
        maxAttempts: 3,        // Maximum retry attempts
        delay: 1000            // Delay between retries (ms)
    }
};
```

## Caching Behavior

The backend implements intelligent caching to optimize performance:

### System Prompt Caching
- System prompts are hashed using MD5
- Only sent to client when the hash changes
- Reduces unnecessary data transfer for repeated requests

### Tools Caching
- Tool definitions are hashed and cached
- Only sent when the tools list changes
- Independent of system prompt caching

### Cache Invalidation
- Cache is per-client (stored in `ClientConnection` object)
- Invalidated when client disconnects
- New connections start with empty cache

## Critical Rules

1. **Preserve OpenAI API compatibility**: Request/response shapes must match OpenAI spec
2. **Never edit js/main.js directly**: Always edit source modules and rebuild
3. **Verify build output**: Check that critical methods exist after build
4. **Feature parity**: Refactoring must not break existing functionality
5. **Platform-specific code is fragile**: Be extra careful with site selectors and conditional logic
6. **Test order matters**: Always build and test JS changes before running backend tests. Wait for user confirmation that browser has been refreshed with new userscript.

## Troubleshooting

### Common Issues and Solutions

#### Backend Issues

**Issue: "No available client connections" error**
- **Cause**: No browser client connected to WebSocket
- **Solution**: Ensure Tampermonkey userscript is installed and browser is on a supported AI platform
- **Verify**: Check `/stats` endpoint for active connections

**Issue: WebSocket connection fails**
- **Cause**: Backend server not running or wrong WebSocket URL
- **Solution**:
  - Run `python backend/main.py` to start server
  - Check `wsServer` in `js/src/modules/config.js` matches backend URL
  - Check browser console for connection errors

**Issue: Request timeout (504)**
- **Cause**: Client didn't respond within 120 seconds
- **Solution**:
  - Check if AI platform is responsive
  - Verify selectors in config.js are correct for current site
  - Check debug logs in `debug_logs/` for details

#### Frontend Issues

**Issue: Userscript not connecting**
- **Cause**: WebSocket server not running or URL mismatch
- **Solution**:
  - Verify backend is running on `localhost:8000`
  - Check browser console for WebSocket errors
  - Ensure `wsServer` in config.js is correct

**Issue: Can't find input/send elements**
- **Cause**: CSS selectors outdated for current platform
- **Solution**:
  - Open browser DevTools and inspect actual DOM structure
  - Update selectors in `js/src/modules/config.js`
  - Rebuild and redeploy userscript

**Issue: Messages not being sent**
- **Cause**: Page not fully loaded or selectors wrong
- **Solution**:
  - Wait for page to fully load
  - Check `pageReadyIndicator` in config.js
  - Review debug logs for specific errors

#### Testing Issues

**Issue: Backend tests fail**
- **Cause**: Browser doesn't have latest userscript
- **Solution**:
  - Rebuild frontend: `python build-main.py`
  - Copy `js/main.js` to Tampermonkey
  - Refresh browser page
  - Wait for user confirmation before running tests

**Issue: Frontend tests fail**
- **Cause**: Node.js dependencies not installed or Jest configuration issues
- **Solution**:
  - Run `cd js && npm install`
  - Check Jest configuration in `package.json`
  - Verify test files in `js/src/tests/`

### Debugging Tips

1. **Enable verbose logging**: Check browser console for frontend errors
2. **Review debug logs**: Check `debug_logs/` for request/response details
3. **Use /stats endpoint**: Monitor connection status during development
4. **Test incrementally**: Test each module separately before integration
5. **Check network tab**: Use browser DevTools Network tab to see WebSocket messages

## Performance Considerations

### Backend Performance
- Use async/await for all I/O operations
- Implement connection pooling for WebSocket clients
- Cache system prompts and tool definitions to reduce bandwidth
- Set appropriate timeouts to prevent hanging requests

### Frontend Performance
- Minimize DOM queries by caching elements
- Use MutationObserver efficiently (don't observe entire document)
- Debounce rapid events (like input changes)
- Clean up event listeners when not needed

### Network Optimization
- System prompt and tools caching reduces redundant data transfer
- Only send necessary messages (filter out old system messages)
- Use WebSocket for real-time communication (lower overhead than HTTP polling)

## Security Considerations

### Backend Security
- CORS is currently set to allow all origins (`*`) - restrict in production
- Validate all incoming requests with Pydantic models
- Implement rate limiting for API endpoints
- Sanitize debug logs to prevent sensitive data leakage
- Use HTTPS/WSS in production environments

### Frontend Security
- Validate WebSocket server URL before connecting
- Sanitize user input before inserting into DOM
- Be careful with `innerHTML` - use `textContent` when possible
- Don't expose sensitive information in console logs

### Data Privacy
- Debug logs may contain sensitive user data - clear regularly
- Consider implementing log rotation for production
- Don't log full message content in production unless necessary
